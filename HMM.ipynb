{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open('cricketbat.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(lines)\n",
    "tokens = [token.lower() for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BAD_CHARS = [';', ':', '!', \"*\", '<', '>','#','?','@','p',',','.','(',')']\n",
    "no_bad_chars = list(filter(lambda token: token not in BAD_CHARS, tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "no_stop_words = list(filter(lambda token: token not in STOP_WORDS, no_bad_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD              STEM              LEMMA\n",
      "----------------  ----------------  ----------------\n",
      "blade             blade             blade\n",
      "cricket           cricket           cricket\n",
      "bat               bat               bat\n",
      "wooden            wooden            wooden\n",
      "block             block             block\n",
      "generally         gener             generally\n",
      "flat              flat              flat\n",
      "striking          strike            striking\n",
      "face              face              face\n",
      "ridge             ridg              ridge\n",
      "reverse           revers            reverse\n",
      "back              back              back\n",
      "concentrates      concentr          concentrate\n",
      "wood              wood              wood\n",
      "middle            middl             middle\n",
      "ball              ball              ball\n",
      "generally         gener             generally\n",
      "hit               hit               hit\n",
      "bat               bat               bat\n",
      "traditionally     tradit            traditionally\n",
      "made              made              made\n",
      "willow            willow            willow\n",
      "wood              wood              wood\n",
      "specifically      specif            specifically\n",
      "variety           varieti           variety\n",
      "white             white             white\n",
      "willow            willow            willow\n",
      "called            call              called\n",
      "cricket           cricket           cricket\n",
      "bat               bat               bat\n",
      "willow            willow            willow\n",
      "salix             salix             salix\n",
      "alba              alba              alba\n",
      "var               var               var\n",
      "caerulea          caerulea          caerulea\n",
      "treated           treat             treated\n",
      "raw               raw               raw\n",
      "unboiled          unboil            unboiled\n",
      "linseed           linse             linseed\n",
      "oil               oil               oil\n",
      "protective        protect           protective\n",
      "function          function          function\n",
      "variety           varieti           variety\n",
      "willow            willow            willow\n",
      "used              use               used\n",
      "tough             tough             tough\n",
      "shock-resistant   shock-resist      shock-resistant\n",
      "significantly     significantli     significantly\n",
      "dented            dent              dented\n",
      "splintering       splinter          splintering\n",
      "impact            impact            impact\n",
      "cricket           cricket           cricket\n",
      "ball              ball              ball\n",
      "high              high              high\n",
      "speed             speed             speed\n",
      "also              also              also\n",
      "light             light             light\n",
      "weight            weight            weight\n",
      "face              face              face\n",
      "bat               bat               bat\n",
      "often             often             often\n",
      "covered           cover             covered\n",
      "protective        protect           protective\n",
      "film              film              film\n",
      "user              user              user\n",
      "blade             blade             blade\n",
      "connected         connect           connected\n",
      "long              long              long\n",
      "cylindrical       cylindr           cylindrical\n",
      "cane              cane              cane\n",
      "handle            handl             handle\n",
      "similar           similar           similar\n",
      "mid-20th-century  mid-20th-centuri  mid-20th-century\n",
      "tennis            tenni             tennis\n",
      "racquet           racquet           racquet\n",
      "means             mean              mean\n",
      "splice            splice            splice\n",
      "handle            handl             handle\n",
      "usually           usual             usually\n",
      "covered           cover             covered\n",
      "rubber            rubber            rubber\n",
      "grip              grip              grip\n",
      "bats              bat               bat\n",
      "incorporate       incorpor          incorporate\n",
      "wooden            wooden            wooden\n",
      "spring            spring            spring\n",
      "design            design            design\n",
      "handle            handl             handle\n",
      "meets             meet              meet\n",
      "blade             blade             blade\n",
      "current           current           current\n",
      "design            design            design\n",
      "cane              cane              cane\n",
      "handle            handl             handle\n",
      "spliced           splice            spliced\n",
      "willow            willow            willow\n",
      "blade             blade             blade\n",
      "tapered           taper             tapered\n",
      "splice            splice            splice\n",
      "invention         invent            invention\n",
      "1880s             1880              1880s\n",
      "charles           charl             charles\n",
      "richardson        richardson        richardson\n",
      "pupil             pupil             pupil\n",
      "brunel            brunel            brunel\n",
      "first             first             first\n",
      "chief             chief             chief\n",
      "engineer          engin             engineer\n",
      "severn            severn            severn\n",
      "railway           railway           railway\n",
      "tunnel            tunnel            tunnel\n",
      "spliced           splice            spliced\n",
      "handles           handl             handle\n",
      "used              use               used\n",
      "tended            tend              tended\n",
      "break             break             break\n",
      "corner            corner            corner\n",
      "join              join              join\n",
      "taper             taper             taper\n",
      "provides          provid            provides\n",
      "gradual           gradual           gradual\n",
      "transfer          transfer          transfer\n",
      "load              load              load\n",
      "bat               bat               bat\n",
      "'s                's                's\n",
      "blade             blade             blade\n",
      "handle            handl             handle\n",
      "avoids            avoid             avoids\n",
      "problem           problem           problem\n",
      "edges             edg               edge\n",
      "blade             blade             blade\n",
      "closest           closest           closest\n",
      "handle            handl             handle\n",
      "known             known             known\n",
      "shoulders         shoulder          shoulder\n",
      "bat               bat               bat\n",
      "bottom            bottom            bottom\n",
      "blade             blade             blade\n",
      "known             known             known\n",
      "toe               toe               toe\n",
      "bat               bat               bat\n",
      "bats              bat               bat\n",
      "always            alway             always\n",
      "shape             shape             shape\n",
      "18th              18th              18th\n",
      "century           centuri           century\n",
      "bats              bat               bat\n",
      "tended            tend              tended\n",
      "shaped            shape             shaped\n",
      "similarly         similarli         similarly\n",
      "modern            modern            modern\n",
      "hockey            hockey            hockey\n",
      "sticks            stick             stick\n",
      "may               may               may\n",
      "well              well              well\n",
      "legacy            legaci            legacy\n",
      "game              game              game\n",
      "'s                's                's\n",
      "reputed           reput             reputed\n",
      "origins           origin            origin\n",
      "although          although          although\n",
      "first             first             first\n",
      "forms             form              form\n",
      "cricket           cricket           cricket\n",
      "obscure           obscur            obscure\n",
      "may               may               may\n",
      "game              game              game\n",
      "first             first             first\n",
      "played            play              played\n",
      "using             use               using\n",
      "shepherd          shepherd          shepherd\n",
      "'s                's                's\n",
      "crooks            crook             crook\n",
      "evolution         evolut            evolution\n",
      "cricket           cricket           cricket\n",
      "bat               bat               bat\n",
      "bat               bat               bat\n",
      "generally         gener             generally\n",
      "recognised        recognis          recognised\n",
      "oldest            oldest            oldest\n",
      "bat               bat               bat\n",
      "still             still             still\n",
      "existence         exist             existence\n",
      "dated             date              dated\n",
      "1729              1729              1729\n",
      "display           display           display\n",
      "sandham           sandham           sandham\n",
      "room              room              room\n",
      "oval              oval              oval\n",
      "london            london            london\n",
      "cricket           cricket           cricket\n",
      "bat               bat               bat\n",
      "specialised       specialis         specialised\n",
      "piece             piec              piece\n",
      "equipment         equip             equipment\n",
      "used              use               used\n",
      "batsmen           batsmen           batsman\n",
      "sport             sport             sport\n",
      "cricket           cricket           cricket\n",
      "hit               hit               hit\n",
      "ball              ball              ball\n",
      "typically         typic             typically\n",
      "consisting        consist           consisting\n",
      "cane              cane              cane\n",
      "handle            handl             handle\n",
      "attached          attach            attached\n",
      "flat-fronted      flat-front        flat-fronted\n",
      "willow-wood       willow-wood       willow-wood\n",
      "blade             blade             blade\n",
      "length            length            length\n",
      "bat               bat               bat\n",
      "may               may               may\n",
      "38                38                38\n",
      "inches            inch              inch\n",
      "965               965               965\n",
      "mm                mm                mm\n",
      "width             width             width\n",
      "4.25              4.25              4.25\n",
      "inches            inch              inch\n",
      "108               108               108\n",
      "mm                mm                mm\n",
      "use               use               use\n",
      "first             first             first\n",
      "mentioned         mention           mentioned\n",
      "1624              1624              1624\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "lemmatize() missing 1 required positional argument: 'word'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-0ce85ba6f819>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtabulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_stop_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'WORD'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'STEM'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LEMMA'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: lemmatize() missing 1 required positional argument: 'word'"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tabulate import tabulate\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stem = [stemmer.stem(token) for token in no_stop_words]\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemma = [lemmatizer.lemmatize(token) for token in no_stop_words]\n",
    "\n",
    "print(tabulate(zip(no_stop_words, stem, lemma), headers=['WORD', 'STEM', 'LEMMA']))\n",
    "\n",
    "lemmatizer.lemmatize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "pos_tags = list(pos_tag(no_stop_words))\n",
    "print(pos_tags[:5])\n",
    "\n",
    "word, tags = zip(*pos_tags)\n",
    "print(tabulate(zip(no_stop_words, stem, lemma, tags), headers=['WORD', 'STEM', 'LEMMA', 'POS_TAG']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "uniGram = Counter(tokens)\n",
    "biGram = Counter(zip(tokens, tokens[1:]))\n",
    "\n",
    "def calculate_probability(sentence, uniGram, biGram):\n",
    "    output_probability = 1\n",
    "    \n",
    "    for word1, word2 in zip(sentence, sentence[1:]):\n",
    "        probability = biGram[(word1, word2)] / uniGram[word1]\n",
    "        output_probability *= probability\n",
    "    \n",
    "    return output_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import RegexpParser\n",
    "\n",
    "chunker = RegexpParser(\"\"\"\n",
    "    NP: {<DT>?<JJ>*<NN>}\n",
    "    P: {<IN>}\n",
    "    V: {<V>.*}\n",
    "    PP: {<P> <NP>}\n",
    "    VP: {<V> <NP|PP>*}\n",
    "\"\"\")\n",
    "\n",
    "  \n",
    "output = chunker.parse(pos_tags)\n",
    "output.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "word = \"sample\"\n",
    "\n",
    "for synonym in wordnet.synsets(word):\n",
    "    print(synonym)\n",
    "    print(synonym.definition())\n",
    "    print(synonym.examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def preprocess(lines):\n",
    "    tokens = word_tokenize(lines)\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    \n",
    "    no_bad_chars = list(filter(lambda token: token not in BAD_CHARS, tokens))\n",
    "    no_stop_words = list(filter(lambda token: token not in STOP_WORDS, no_bad_chars))\n",
    "    \n",
    "    return no_stop_words\n",
    "\n",
    "def get_word_lemmas(tokens):\n",
    "    word_lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return word_lemmas\n",
    "\n",
    "def get_synonyms_lemma(word):\n",
    "    synonyms = []\n",
    "\n",
    "    for synonym in wordnet.synsets(word):\n",
    "        synonyms += [lemma.name() for lemma in synonym.lemmas()]\n",
    "\n",
    "    return synonyms\n",
    "\n",
    "def get_word_and_syn_lemmas(tokens):\n",
    "    word_and_syn_lemmas = []    # Should contain the lemma of the word and its synonyms\n",
    "\n",
    "    for word in tokens:\n",
    "        word_and_syn_lemmas.append(lemmatizer.lemmatize(word))   # Adding the lemma of the word\n",
    "        word_and_syn_lemmas.extend(get_synonyms_lemma(word))     # Adding the lemma of all the synonyms of the word\n",
    "    \n",
    "    return word_and_syn_lemmas\n",
    "\n",
    "def words_simlilarity_score(word1, word2):\n",
    "    word1 = word1 + \".n.01\"\n",
    "    word2 = word2 + \".n.01\"\n",
    "\n",
    "    try:\n",
    "        w1 = wordnet.synset(word1)\n",
    "        w2 = wordnet.synset(word2)\n",
    "        return w1.wup_similarity(w2)\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_sentence = \"Will be given by the user\"\n",
    "file_1 = open('cricketbat.txt').read() \n",
    "file_2 = open('vampirebat.txt').read()\n",
    "\n",
    "tokens1 = preprocess(file_1) \n",
    "tokens2 = preprocess(file_2) \n",
    "tokens3 = preprocess(query_sentence)\n",
    "\n",
    "\n",
    "word_lemmas_1 = get_word_lemmas(tokens1)\n",
    "word_syn_lemmas_1 = get_word_and_syn_lemmas(tokens1)\n",
    "\n",
    "word_lemmas_2 = get_word_lemmas(tokens2)\n",
    "word_syn_lemmas_2 = get_word_and_syn_lemmas(tokens2)\n",
    "\n",
    "word_lemmas_3 = get_word_lemmas(tokens3)\n",
    "word_syn_lemmas_3 = get_word_and_syn_lemmas(tokens3)\n",
    "\n",
    "exact_word_match13 = 0\n",
    "exact_word_match23 = 0\n",
    "for word3 in word_lemmas_3:\n",
    "    for word1 in word_lemmas_1:\n",
    "        exact_word_match13 += 1 if word1 == word3 else 0\n",
    "\n",
    "    for word2 in word_lemmas_2:\n",
    "        exact_word_match23 += 1 if word2 == word3 else 0\n",
    "\n",
    "similarity_score13 = 0\n",
    "similarity_score23 = 0\n",
    "for word3 in word_lemmas_3:\n",
    "    for word1 in word_lemmas_1:\n",
    "        similarity_score13 += words_simlilarity_score(word3, word1)\n",
    "\n",
    "    for word2 in word_lemmas_2:\n",
    "        similarity_score23 += words_simlilarity_score(word3, word2)\n",
    "\n",
    "file1_score = exact_word_match13 + similarity_score13\n",
    "file2_score = exact_word_match23 + similarity_score23\n",
    "\n",
    "if file1_score > file2_score: print(\"The query sentence belongs to file1\")\n",
    "else: print(\"The query sentence belongs to file2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What follows is for HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import math\n",
    "from itertools import chain\n",
    "\n",
    "TRAIN_PERCENT = 0.7\n",
    "\n",
    "training_corpus = nltk.corpus.brown\n",
    "tagged_sents = training_corpus.tagged_sents()\n",
    "\n",
    "X, y = [], []\n",
    "for sentence in tagged_sents:\n",
    "    x_, y_ = list(zip(*sentence))\n",
    "    X += x_\n",
    "    y += y_\n",
    "\n",
    "train_size = math.ceil(TRAIN_PERCENT * len(X))\n",
    "\n",
    "X_train = X[:train_size]\n",
    "y_train = y[:train_size]\n",
    "X_test = X[train_size:]\n",
    "y_test = y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vocab = frozenset(chain([s for s in X_train]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pomegranate import State,HiddenMarkovModel,DiscreteDistribution\n",
    "model = HiddenMarkovModel(name=\"HMM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "def get_unigram(sequences):\n",
    "    return Counter(chain(sequences))\n",
    "\n",
    "def get_bigram(sequences):\n",
    "    bigrams = defaultdict(int)\n",
    "    \n",
    "    for sequence in sequences:\n",
    "        for i in zip(sequence, sequence[1:]): \n",
    "            bigrams[i] += 1\n",
    "    \n",
    "    return bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tags = set(y)\n",
    "tag_unigrams = get_unigram(y_train)\n",
    "tag_bigrams = get_bigram(y_train)\n",
    "\n",
    "emission_counts = defaultdict(lambda: defaultdict(int))\n",
    "for word, tag in zip(X_train, y_train):\n",
    "    emission_counts[tag][word] += 1\n",
    "\n",
    "emissions_distribution = {}\n",
    "states = dict()\n",
    "for tag in unique_tags:\n",
    "    for word in emission_counts[tag]:\n",
    "        emissions_distribution[word] = emission_counts[tag][word] / tag_unigrams[tag]\n",
    "    tag_emissions = DiscreteDistribution(emissions_distribution)\n",
    "    tag_state = State(tag_emissions, name=tag)\n",
    "    states[tag]=tag_state\n",
    "\n",
    "model.add_states([elt for elt in states.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_state_count=defaultdict(int)\n",
    "for sentence in tagged_sents:\n",
    "    start_state_count[sentence[0][1]] +=1\n",
    "\n",
    "tag_starts_sum=sum(start_state_count.values())\n",
    "for tag in unique_tags:    \n",
    "    prob = start_state_count[tag] / tag_starts_sum\n",
    "    model.add_transition(model.start, states[tag], prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_state_count=defaultdict(int)\n",
    "for sentence in tagged_sents:\n",
    "    stop_state_count[sentence[-1][1]] +=1\n",
    "\n",
    "tag_stop_sum=sum(stop_state_count.values())\n",
    "for tag in unique_tags:    \n",
    "    prob = stop_state_count[tag] / tag_stop_sum\n",
    "    model.add_transition(states[tag], model.end, prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t1, t2 in tag_bigrams.keys():\n",
    "    # Compute the transition probability P(t2|t1)=C(t1,t2) / C(t1)\n",
    "    if tag_unigrams[t1] <= 0: continue\n",
    "    prob = tag_bigrams[(t1,t2)] / tag_unigrams[t1]\n",
    "    model.add_transition(states[t1], states[t2] , prob)\n",
    "model.bake()\n",
    "_, state_path = model.viterbi(X_test)\n",
    "output_sequence=[state[1].name for state in state_path[1:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python383jvsc74a57bd0148be82d34653dcc0aaf818b747d874446b9d01bf97e708160063c3df3e04544"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
